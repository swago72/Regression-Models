{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and specify that graphs should be plotted inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# warnings reported for function updates, ignore them\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practice, we implement three advanced regression models: Polynomial Regression, Ridge Regression, and LASSO Regression. \n",
    "\n",
    "**Note: For all the model-relevant syntax, you can google the syntax (in bold) and get its manual.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Polynomial Regression\n",
    "### 1.1 Polynomial Regression Basics\n",
    "In a polynomial regression, the relationship between $y$ and $x$ is modeled as \"$k$<sup>th</sup> degree polynomial\" in $x$. \n",
    "\n",
    "For $k$<sup>th</sup> degree polynomial, the model is shown as:\n",
    "\n",
    "<center>$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ... + \\beta_k x^k + \\varepsilon$</center>\n",
    "\n",
    "**Polynomial with Multiple Variables**\n",
    "\n",
    "Note that when we have multiple variables, say, x1 and x2, the polynomials would be all potential combinations of x1 and x2. For example, a model with 2nd degree polynomial for (x1, x2) would be:\n",
    "\n",
    "\n",
    "<center>$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_2 + \\beta_4 x_2^2 + \\beta_5 x_1 x_2 + \\varepsilon$</center>\n",
    "\n",
    "It is obvious that when the degree is higher, and when we have more variables, writing out the polynomials would be extremely tedious. (Fortunately, we do not need to generate the polynomials ourselves in Python.)\n",
    "\n",
    "\n",
    "### Data Loading and Splitting\n",
    "As multiple variables will be created in polynomial regression, we use a single input variable $x$ for simplicity. We will use the same dataset in the previous lecture, \"house.csv\". The dependent variable is **'TOTAL_VALUE'**. The independent variable is **'LOT_SQFT'**.\n",
    "\n",
    "**Practice:** \n",
    "- Let dependent variable be **'TOTAL_VALUE'**. Let independent variable be **'LOT_SQFT'**. \n",
    "    - Note: Use Series.to_frame() method to convert Series to DataFrame (i.e., 1D to 2D)\n",
    "- Split the data into 70% training and 30% test set. Set seed (random_state) = 42.\n",
    "- Check sample size of training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5802,)\n",
      "(5802, 1)\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "house = pd.read_csv('C:\\\\Users\\\\raksh\\\\OneDrive\\\\Desktop\\\\Sem 3\\\\AML\\\\Class 4\\\\house.csv')\n",
    "\n",
    "# Define X and Y Below, print the shape of house_X\n",
    "house_X = house['LOT_SQFT']\n",
    "house_y = house['TOTAL_VALUE']\n",
    "print(house_X.shape)\n",
    "\n",
    "# Note that house_X is a 1D array, which cannot fit in sklearn models as x.\n",
    "house_X = house_X.to_frame()\n",
    "\n",
    "print(house_X.shape) # Now X is 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitting, check sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(house_X, house_y, \n",
    "                                                    test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Polynomial Regression with Scikit-Learn\n",
    "\n",
    "Estimating polynomial regression takes one extra step compared to linear regression. The reason is that generating polynomials is tedious, and we need a specific function to complete this step. Thus, the first step is to generate polynomials, and the second step is to run the linear regression.\n",
    "\n",
    "Specifically, the two steps are: \n",
    "\n",
    "- First, specify the degree of polynomial regression (i.e., speicfy $k$). Generate variables based on the specified degree. This step is done by creating a new polynomial feature object using syntax: \n",
    "<br> <center><span style=\"font-family:Calibri\"> **sklearn.preprocessing.PolynomialFeatures(degree)** </span></center>\n",
    "    - Use .fit_transform(X) method the get the transfered features.\n",
    "    - You need to specify the degree (i.e., degree = xxx) before training.\n",
    "\n",
    "- Second, run a linear regression based on the features generated in the first step. This step is done by: \n",
    "<br> <center><span style=\"font-family:Calibri\"> **sklearn.linear_model.LinearRegression()** </span></center>\n",
    "    - Recall that we have learned its features & functions: .fit, .predict, .score, .coef_, .intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**\n",
    "- Assume $k=3$. Run polynomial regression. (Hint: first create the polynomial features, then run the linear regression).\n",
    "- Obtain and report the mse for test set. (Hint: You need to generate polynomial features for test set to do predictions.)\n",
    "- Obtain and report the coefficient estimates (include and specify intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S1. Obtain Polynomial Features \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "## S1.1 Define polynomial generation function and set the degree. Change x to x^0, x, x^2, x^3\n",
    "poly = PolynomialFeatures(degree = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4061, 4)\n"
     ]
    }
   ],
   "source": [
    "# S1.2 Obtain the variables: Which set?\n",
    "\n",
    "X_poly_train = poly.fit_transform(X_train)\n",
    "print(X_poly_train.shape) # include x^0: = 1 => constant term / intercept\n",
    "\n",
    "# Also transfer for test set => for model evaluation and prediction\n",
    "X_poly_test = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S2. Run Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# S2.1, define the linear regression function (plug in x^0, x, x^2, x^3, then obtain betas)\n",
    "lr = LinearRegression()\n",
    "## S2.2, train the model\n",
    "lr.fit(X_poly_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7045.946151190245"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S3. Predict and calculate error\n",
    "y_pred_test = lr.predict(X_poly_test)\n",
    "\n",
    "## Calculate mse\n",
    "e = y_test - y_pred_test\n",
    "MSE = np.mean(e**2)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00000000e+00,  4.78463125e-02, -1.95203304e-06,  2.87628127e-11]),\n",
       " 169.33378283967744)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S4. Report estimates\n",
    "lr.coef_, lr.intercept_\n",
    "# first coef (x^0 = 1) is zero, \n",
    "## it is the intercept, which duplicates with lr.intercept_ separately estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Pipelines (Technical Pre-requisite to Obtain Optimal Degree)\n",
    "\n",
    "We need to define two functions that comes in a specific order to complete the previous task. In practice, it is recommended to use a \"pipeline\" to automate this process.\n",
    "\n",
    "To put two or multiple steps (e.g., PolynomialFeatures and LinearRegression) together with a specific sequence, we create a pipeline object using syntax:\n",
    "<br> <center><span style=\"font-family:Calibri\"> **sklearn.pipeline.make_pipeline()** </span></center>\n",
    "- The inputs are the models used in the process. The order of input should be the order of your models/objects.\n",
    "- The pipeline object will replace your original model for estimation. You can imagine that make_pipeline is putting your models in a bucket following a specific order.\n",
    "\n",
    "Similar to other models, we can still use  .fit, .fit_transform, .score, .predict and so on.\n",
    "\n",
    "Specific to make_pipeline(), we can use **.named_steps** to obtain the models inside the bucket. If the model is trained, then all the necessary information can be accessed as well (e.g., coefficients, intercept, etc.) To specify which model you want to look into, use ['Python_Defined_Model_Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice: Using make_pipeline for simple polynomial regression**\n",
    "\n",
    "Use make_pipeline to run the polynomial regression with degree = 3. \n",
    "- Define the two steps first. Then put them in a pipeline.\n",
    "- Train the model using the pipeline you created.\n",
    "- Obtain the mse for test set.\n",
    "- Compare the process with the previous practice, what are the differences in the progress?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7045.946151190245 7045.946151190245\n"
     ]
    }
   ],
   "source": [
    "### In below, use make_pipeline for practice\n",
    "from sklearn.preprocessing  import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# S1. Define models - two models, polynomial transfer first, then linear regression \n",
    "poly_transfer = PolynomialFeatures(degree = 3)\n",
    "my_lr = LinearRegression()\n",
    "\n",
    "# S2. Apply polynomial regression in pipeline\n",
    "my_poly_reg = make_pipeline(poly_transfer, my_lr)\n",
    "my_poly_reg.fit( X_train  , y_train)\n",
    "\n",
    "# S3. Predict and get mse\n",
    "y_poly_test_pred = my_poly_reg.predict(X_test)\n",
    "MSE_poly = np.mean((y_test - y_poly_test_pred)**2)\n",
    "print(MSE_poly, MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice: Getting results from make_pipeline**\n",
    "- Obtain and report the coefficients, and the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'polynomialfeatures': PolynomialFeatures(degree=3),\n",
       " 'linearregression': LinearRegression()}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_poly_reg.named_steps \n",
    "# Dictionary => {} , \n",
    "# Each element, \":\", before :, always a string => key\n",
    "#                    after :, whatever value/function/object/..., stored under the key\n",
    "# Access information based on key value, spelling must match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_poly_reg.named_steps['linearregression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  4.78463125e-02, -1.95203304e-06,  2.87628127e-11])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_poly_reg.named_steps['linearregression'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169.33378283967744"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_poly_reg.named_steps['linearregression'].intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Hyperparameter Tuning with Polynomial Regression\n",
    "In the previous case, we consider a naive scenario where k = 3. Recall that k is the hyperparameter. To get a better prediction result, we may consider tune the degree k to find the k that gives the best performance.\n",
    "\n",
    "To make best use of our data, and to avoid overfitting, we will apply cross-validation for performance measure. The best k should be chosen based on the (mean) performance of the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sklearn provides a nice syntax that combines grid search and cross validation:\n",
    "<br> <center><span style=\"font-family:Calibri\"> **sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring,  cv)** </span></center>\n",
    "\n",
    "- estimator : the model. If a sequence of models, then use pipeline to put them together.\n",
    "- param_grid : dictionary format. Specifies the potential choice of parameters. The keys must be correct.\n",
    "- scoring : Performance measure. For linear models, default is R-square. Can be specified to mae (neg_mean_absolute_error), mse ('neg_mean_squared_error') as well.\n",
    "- cv : Determines the cross-validation splitting strategy. If cv is integer (say, k), then k-fold cv. Default is 5-fold cv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice: Use GridSearchCV with polynomial regression - Train and Predict**\n",
    "\n",
    "Apply grid search for hyperparameter tuning, and select the best model based on cross-validation performance. Use R-square as the performance measure. Potential candidate of hyperparameter: integers from 1 to 5, include 1 and 5.\n",
    "- Define grid of hyperparameters.\n",
    "- Define the estimator.\n",
    "- Define gridsearchCV\n",
    "- Train the model. What is the test mse? Is the model chosen based on the test mse?\n",
    "- By default, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gridsearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load other modules\n",
    "from sklearn.preprocessing  import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# S1. Define grid of parameter. \n",
    "## The name should be exactly the same, otherwise cannot find which to specify.\n",
    "\n",
    "my_poly_grids = { 'polynomialfeatures__degree'  : [1, 2, 3, 4, 5] }\n",
    "# param_poly = {'polynomialfeatures__degree' :  range(1,6)  }\n",
    "## range function to get consecutive integers: range(i, j) start i, stop j, exclude j\n",
    "\n",
    "# S2. Define estimator: use make_pipeline to combine two functions. \n",
    "poly_grid = PolynomialFeatures()\n",
    "my_lr_grid = LinearRegression()\n",
    "\n",
    "# S2. Apply polynomial regression in pipeline\n",
    "my_poly_reg_grid = make_pipeline(poly_grid, my_lr_grid)\n",
    "\n",
    "# S3. Define GridSearchCV Estimation function, then train the model\n",
    "grid_polyreg = GridSearchCV( my_poly_reg_grid, my_poly_grids, cv = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('polynomialfeatures',\n",
       "                                        PolynomialFeatures()),\n",
       "                                       ('linearregression',\n",
       "                                        LinearRegression())]),\n",
       "             param_grid={'polynomialfeatures__degree': [1, 2, 3, 4, 5]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_polyreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([372.368752  , 336.66401182, 365.97019729, ..., 304.04738548,\n",
       "       455.7230367 , 363.54271511])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_polyreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3235362995692006"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_polyreg.score(X_test, y_test) # this is the unbiased performance measure\n",
    "# this is not validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outputs of GridSearchCV:**\n",
    "- best_score_ : the mean validation score of the best model. The best model's performance measure, based on which the model is chosen.\n",
    "- best_params_: the choice of hyperparameter\n",
    "- best_estimator_: the best model of choice (and corresponding results). You can access model estimates from this attribute.\n",
    "- cv_results_: detailed results stored (e.g., time & score of each hyperparameter, each iteration). Dictionary format.\n",
    "\n",
    "**Practice: Collect Results from GridSearchCV**\n",
    "- What is the chosen degree of polynomial regression?\n",
    "- For the best model, report its performance score based on which the model is chosen.\n",
    "- Explore attribute: cv_results_. Can you provide evidence of why the best model should be chosen? \n",
    "- Explore attribute:best_estimator_. Under the chosen model, what are the coefficients (include intercept)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'polynomialfeatures__degree': 3}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. Chosen degree: this is the ...?\n",
    "\n",
    "grid_polyreg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31736242882259663"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2. The performance is chosen based on ...?\n",
    "\n",
    "grid_polyreg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00120006, 0.00153847, 0.00261698, 0.00061245, 0.00281796]),\n",
       " 'std_fit_time': array([0.00146976, 0.00137979, 0.00043465, 0.00082091, 0.00563593]),\n",
       " 'mean_score_time': array([0.00112419, 0.0037858 , 0.00097365, 0.00313354, 0.        ]),\n",
       " 'std_score_time': array([1.20598233e-03, 6.07142403e-03, 3.74477802e-05, 6.26707077e-03,\n",
       "        0.00000000e+00]),\n",
       " 'param_polynomialfeatures__degree': masked_array(data=[1, 2, 3, 4, 5],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'polynomialfeatures__degree': 1},\n",
       "  {'polynomialfeatures__degree': 2},\n",
       "  {'polynomialfeatures__degree': 3},\n",
       "  {'polynomialfeatures__degree': 4},\n",
       "  {'polynomialfeatures__degree': 5}],\n",
       " 'split0_test_score': array([0.28503762, 0.31455706, 0.32289995, 0.32421681, 0.32007107]),\n",
       " 'split1_test_score': array([0.26901215, 0.30245627, 0.30871849, 0.30690419, 0.29903995]),\n",
       " 'split2_test_score': array([0.30160033, 0.32395589, 0.33977123, 0.34045278, 0.33948694]),\n",
       " 'split3_test_score': array([0.25648781, 0.29758192, 0.30113049, 0.30301227, 0.28982309]),\n",
       " 'split4_test_score': array([ 0.35131823,  0.24765355,  0.31429198,  0.27603391, -0.56614045]),\n",
       " 'mean_test_score': array([0.29269123, 0.29724094, 0.31736243, 0.31012399, 0.13645612]),\n",
       " 'std_test_score': array([0.03300443, 0.02645661, 0.01326981, 0.02164034, 0.35171944]),\n",
       " 'rank_test_score': array([4, 3, 1, 2, 5])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3. Check cv_results_\n",
    "\n",
    "grid_polyreg.cv_results_ #Lookout for 'mean_test_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29269123, 0.29724094, 0.31736243, 0.31012399, 0.13645612])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_polyreg.cv_results_['mean_test_score'] #lookout for largest mean validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31736242882259663"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(grid_polyreg.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  4.78463125e-02, -1.95203304e-06,  2.87628127e-11])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 4. Check best_estimator_\n",
    "\n",
    "grid_polyreg.best_estimator_.named_steps['linearregression'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169.33378283967744"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_polyreg.best_estimator_.named_steps['linearregression'].intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating ridge regression is done by syntax:\n",
    "<br> <center><span style=\"font-family:Calibri\"> **sklearn.linear_model.Ridge()** </span></center>\n",
    "The hyperparameter is specified as \"alpha\". By default, alpha = 1. *Note that this is the same for almost all models in sklearn.linear_model, including logistic regression*\n",
    "\n",
    "\n",
    "**Practice**\n",
    "- Prepare data as in polynomial case. Let $X$ be variables:  'GROSS_AREA', 'ROOMS', 'LIVING_AREA', 'LOT_SQFT', 'FLOORS', 'FULL_BATH'\n",
    "- Run ridge regression without model selection or CV. Use defalt alpha = 1\n",
    "- Run ridge regression with grid search and CV. Select tuning parameter from: [0.001, 0.01, 0.1, 1, 10,100].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Splitting\n",
    "var = ['GROSS_AREA', 'ROOMS', 'LIVING_AREA', 'LOT_SQFT', 'FLOORS', 'FULL_BATH']\n",
    "X = house[var]\n",
    "y = house['TOTAL_VALUE']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model and prediction\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_base = Ridge(alpha=1)\n",
    "ridge_base.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2305.016177217156"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = ridge_base.predict(X_test)\n",
    "\n",
    "MSE_test = np.mean((y_test-y_test_pred)**2)\n",
    "MSE_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7787011510824884\n",
      "0.783214249774319\n",
      "{'alpha': 1}\n",
      "[3.19378284e-02 7.32326341e-01 5.99725568e-02 8.74173838e-03\n",
      " 4.46372940e+01 1.78431657e+01]\n",
      "41.12660659426007\n"
     ]
    }
   ],
   "source": [
    "## Grid Search with CV\n",
    "\n",
    "# 1. Define a list of parameters (key is 'alpha')\n",
    "\n",
    "param_grid = {'alpha' :  [0.001, 0.01, 0.1, 1, 10,100]}\n",
    "\n",
    "\n",
    "# 2. Define function and fit the data\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "grid_ridge = GridSearchCV(ridge, param_grid, cv=5) # format is GridSearchCV(Estimate, Grid, cv)\n",
    "\n",
    "grid_ridge.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 3.1 Present performance measure\n",
    "\n",
    "##unbiased measure in R^2\n",
    "print(grid_ridge.score(X_test, y_test))\n",
    "\n",
    "#mean validation score of the best model\n",
    "print(grid_ridge.best_score_)\n",
    "\n",
    "# 3.2 find best hyperparameters\n",
    "print(grid_ridge.best_params_)\n",
    "\n",
    "# 3.3 find best parameter estimates\n",
    "print(grid_ridge.best_estimator_.coef_)\n",
    "print(grid_ridge.best_estimator_.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO\n",
    "\n",
    "Estimating LASSO regression is done by creating a Lasso object using syntax:\n",
    "<br> <center><span style=\"font-family:Calibri\"> **sklearn.linear_model.Lasso** </span></center>\n",
    "- hyperparameter is also alpha. Default is 1.\n",
    "\n",
    "**Practice**\n",
    "- Prepare data as in polynomial case. Let $X$ be 'GROSS_AREA', 'ROOMS', 'LIVING_AREA', 'LOT_SQFT', 'FLOORS', 'FULL_BATH'.\n",
    "- Run LASSO regression without model selection or CV\n",
    "- Run LASSO regression with grid search and CV. Select tuning parameter from: [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7767894037240006"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model and prediction with default hyperparameter\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_base = Lasso(alpha=1)\n",
    "lasso_base.fit(X_train, y_train)\n",
    "\n",
    "lasso_base.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7787051811426311\n",
      "0.7832136124199647\n",
      "{'alpha': 1e-05}\n",
      "[3.19645211e-02 7.25754110e-01 5.98952703e-02 8.74321785e-03\n",
      " 4.47262062e+01 1.78725451e+01]\n",
      "41.02602342600778\n"
     ]
    }
   ],
   "source": [
    "# Grid Search with CV - LASSO Case\n",
    "\n",
    "# 1. Define a list of parameters (key is 'alpha')\n",
    "\n",
    "param_grid = {'alpha' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "\n",
    "# 2. Define function and fit the data\n",
    "\n",
    "lasso = Lasso()\n",
    "\n",
    "grid_lasso = GridSearchCV(lasso, param_grid, cv=5) # format is GridSearchCV(Estimate, Grid, cv)\n",
    "\n",
    "grid_lasso.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 3.1 Present performance measure\n",
    "\n",
    "##unbiased measure in R^2\n",
    "print(grid_lasso.score(X_test, y_test))\n",
    "\n",
    "#mean validation score of the best model\n",
    "print(grid_lasso.best_score_)\n",
    "\n",
    "# 3.2 find best hyperparameters\n",
    "print(grid_lasso.best_params_)\n",
    "\n",
    "# 3.3 find best parameter estimates\n",
    "print(grid_lasso.best_estimator_.coef_)\n",
    "print(grid_lasso.best_estimator_.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
